\section{Related Works}

After emerging the attention mechanism, this architecture has become critical components for state-of-art neural network models in various applications such as natural language processes, graph networks, and computer vision \cite{Vaswani2017attention}. Stochastic attention methods enable capturing complicated dependencies and regularizing weights based on the user's prior knowledge \cite{shankar2018posterior, lawson2018learning, bahuleyan2018variational, Deng2018Latent}. Majority of stochastic attention focus on hard attention, where the attention weights regard as discrete random variables sampled from categorical distribution. But these methods cannot utilize back-propagation since they do not consider reparameterization trick to draw samples \cite{shankar2018posterior, lawson2018learning, Deng2018Latent}. Even if these methods employ normal distribution as a posterior distribution of latent variables, it was impossible to satisfy the simplex constraints, which do not sum to one \cite{bahuleyan2018variational}. Recently, the bayesian attention module suggests that the attention weights are samples from the dirichlet distribution whose parameters can be obtained by keys and queries and reparameterizable. Accordingly, this method is stable to train and maintains good scalability \cite{Fan2020bayesian}. \\ 
Since conditional neural processes have been proposed, there have been several following works to improve the neural processes in various aspects \cite{garnelo2018conditional}. The neural processes suggested using a global probabilistic latent variable to model functional uncertainty \cite{garnelo2018neural}. The attentive neural processes modify the set encoder as the attention mechanism in the average pooling to increase the performance of the predictability, and explainability \cite{kim2019attentive}. Some papers consider the neural processes for sequential data \cite{yoon2020robustifying, Singh2019Sequential}. Some studies try to combine with optimization meta-learning to increase the performance of neural processes for the image classification task with a pre-trained image encoder \cite{rusu2018meta, xu2020metafun}. The convolutional network can be used for context dataset encoding to have translation-invariant predictions specialized in image-based problems \cite{gordon2019convolutional, Foong2020Meta}. The multi-layer perceptron can also extend to a group-invariant property by the meta-loss derived from the lie group convolution \cite{kawano2021group}. As the similar line of this paper, bayesian context aggregation has been proposed to improve the predictive performance of standard neural processes. \cite{Volpp2021bayesian} This paper also creates a latent variable to represent all tasks, and context embedding can be made using probabilistic sampling with the context dataset. Unfortunately, there is no clear explanation of how stochasticity has helped improve performance, and this method cannot be compatible with attentive neural processes. \\
In this paper, we proposed neural processes with stochastic attention that enhance prediction performance and have better explainability of set encoding by capturing complicated dependencies between context and target dataset under noisy and few-shot settings. In particular, we discover that the stochastic attention mechanism provides appropriate randomness to avoid memorizing the meta-train dataset. 
Consequently, the proposed model better learns the tendency of all tasks during the meta-train step and adequately predicts with consideration of a context dataset compared to existing methods. We can identify that the proposed method achieves meta-learning perspective by experimental results. 

 